# PCA-ICA-LDA-SVD
PCA-ICA-LDA，的数学原理，应用，效果。
我们有时会遇到数据维度比较高，数据维度带来的缺点一个是计算量比较大，而是VC维过大，需要的数据比较多，这在一些数据较难获得
的应用上比较困难，所以需要把维度降下来，就是特征减小些，很自然的想法是就是我要去掉那些比较相关的特征。把比较相关的特征合并成一个，或者说我可以把原数据空间映射到一个子空间，这个子空间包含着原数据最多的不相关的的特征，怎么确定不相关呢，换句话说就是保证差异性，对于一维数据方差表示数据点的差异，但是对于高维数据我们希望在一组基上是方差最大，而两组基上是不想关的，这个有点想协方差矩阵，，协方差主对角线是向量里的方差，对角线时两个向量之间的方差。这是啥，这是矩阵对角化啊，怎么做，特征矩阵的单位正交积，我们矩阵的物理意义，矩阵特征值可以说是矩阵的速度，而特征向量可以看出是矩阵的方向。所以选择矩阵特征值大的能量也会较高，so我怎么求特征向量的单位正交积，然后取特征值比较大的k个就可以了。这样最后做个内积就把数据映射到了最优秀的k维空间内了。应用：降维，去噪，可视化。
下面是降维后的数据。不过假设数据不是高斯分布的那么即使协方差为0也只能说明不相关，协方差也智能描述线性的相关。而不是相互独立。因此若不是高斯分布很难用pca，什么是高斯过程就是取样点满足高斯分布。
图：












LDA：LDA是一种监督学习的降维技术，也就是说它的数据集的每个样本是有类别输出的。这点和PCA不同。PCA是不考虑样本类别输出的无监督降维技术。LDA的思想可以用一句话概括，就是“投影后类内方差最小，类间方差最大”。什么意思呢？ 我们要将数据在低维度上进行投影，投影后希望每一种类别数据的投影点尽可能的接近，而不同类别的数据的类别中心之间的距离尽可能的大。












ICA：ICA只是让输出信号尽量独立，实际应用中因种种因素，比如噪声影响、非线性因素、太多源信号的影响等等，输出往往不是完全独立。这时很多情况下ICA的输出还是包含了主要的独立的成分，是有意义的。最经典的应用就是语音分解，就是输入是由相互独立的信号组成。即还原有规律的信号。详细见这个http://blog.csdn.net/ffeng271/article/details/7353881







SVD：和pca一样可以把数据压缩，这样可以去掉噪声和冗余特征。最经典就是推荐系统里的SVD，假设数据是比较稀疏的那么只要一小快就可以反映出较大的数据信息，其他的可以理解为噪声和不想关的特征，所欲奇异值大的就包含能量大，奇异值晓得就包含能量小。和特征值类似，甚至可以用SVD做PCA，怎么求能量呢，就是平方和。2006大赛就是通过SVD获得了100万美金。

